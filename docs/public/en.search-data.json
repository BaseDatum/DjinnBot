{"/docs/":{"data":{"":"Welcome to the DjinnBot documentation. Everything you need to go from zero to a fully autonomous AI development team.\nGetting StartedInstall, configure, and run your first pipeline in under 5 minutes. Core ConceptsUnderstand agents, pipelines, memory, and the architecture. GuidesStep-by-step walkthroughs for Slack setup, custom agents, and more. ReferenceAPI endpoints, CLI commands, configuration options, and pipeline YAML. AdvancedCustom LLM providers, MCP tools, local models, and scaling."},"title":"Documentation"},"/docs/advanced/":{"data":{"":"Advanced topics for power users â€” custom LLM providers, local models, scaling, and extending DjinnBot.\nLLM ProvidersConfigure any LLM provider including local models and custom endpoints. Bot InterfacesCurrent and future chat interfaces beyond Slack. Local DevelopmentSet up the development environment without Docker. Security ModelContainer isolation, secret management, and access control."},"title":"Advanced"},"/docs/advanced/bot-interfaces/":{"data":{"":"DjinnBot agents can interact through multiple interfaces. Currently Slack and the built-in dashboard chat are supported, with more platforms planned.","api-only#API-Only":"The REST API and SSE streaming already allow building custom frontends. Any application that can make HTTP requests can interact with DjinnBot agents.","architecture-for-new-interfaces#Architecture for New Interfaces":"Adding a new chat interface involves:\nBridge service â€” connects to the external platformâ€™s API (similar to packages/slack/) Event routing â€” maps platform events to DjinnBotâ€™s event bus Per-agent identity â€” manages bot accounts/tokens for each agent Thread mapping â€” links platform threads to pipeline runs The engineâ€™s event-driven architecture makes this straightforward â€” new interfaces subscribe to Redis events and publish commands back.","contributing-an-interface#Contributing an Interface":"If you want to add support for a new platform, the Slack package (packages/slack/) is the reference implementation. The key files:\nslack-bridge.ts â€” routes events between Slack and the engine agent-slack-runtime.ts â€” manages per-agent Socket Mode connections thread-manager.ts â€” maps runs to Slack threads slack-streamer.ts â€” streams agent output to Slack messages A new interface would implement the same patterns for a different platform.","current-interfaces#Current Interfaces":"","custom-webhooks#Custom Webhooks":"A generic webhook interface will allow integration with any chat platform or custom application. Send messages in, receive agent responses out, via simple HTTP.","dashboard-chat#Dashboard Chat":"The built-in chat interface at http://localhost:3000/chat requires no additional setup. Features:\nSelect any agent and model Full tool access (code execution, file operations, web research) Persistent chat history Real-time streaming responses Supports onboarding and project-context sessions This is the primary interface for users who donâ€™t use Slack.","discord#Discord":"Discord bot support is on the roadmap. The architecture mirrors Slack â€” each agent gets its own bot account, pipeline runs create threads, and agents respond to mentions.","microsoft-teams#Microsoft Teams":"Teams integration is planned for enterprise environments that standardize on the Microsoft ecosystem.","planned-interfaces#Planned Interfaces":"","slack#Slack":"Each agent gets its own Slack bot via Socket Mode. See Slack Bot Setup for configuration.\nFeatures:\nPer-agent bot identity (name, avatar) Pipeline threads (watch agents collaborate) Direct mentions and DMs Active/passive thread participation"},"title":"Bot Interfaces"},"/docs/advanced/llm-providers/":{"data":{"":"DjinnBot supports a wide range of LLM providers through pi-mono. You can use cloud APIs, local models, or any OpenAI-compatible endpoint.","cloud-providers#Cloud Providers":"Provider Env Variable Models OpenRouter OPENROUTER_API_KEY All models (Claude, GPT, Gemini, Kimi, Llama, etc.) Anthropic ANTHROPIC_API_KEY Claude Sonnet, Opus, Haiku OpenAI OPENAI_API_KEY GPT-4o, GPT-4, o1, o3 Google GEMINI_API_KEY Gemini 2.5 Pro, Flash xAI XAI_API_KEY Grok 4 Groq GROQ_API_KEY Llama, Mixtral (fast inference) Mistral MISTRAL_API_KEY Mistral Large, Codestral Cerebras CEREBRAS_API_KEY Llama (fast inference) Azure OpenAI AZURE_OPENAI_API_KEY GPT models via Azure Amazon Bedrock AWS credentials Claude, Llama, Titan Google Vertex GCP ADC Gemini, PaLM Hugging Face HF_TOKEN Open models via Inference API","configuring-providers#Configuring Providers":"","custom-providers-openai-compatible#Custom Providers (OpenAI-Compatible)":"You can add any OpenAI-compatible endpoint as a custom provider through the dashboard:\nGo to Settings â†’ Providers Click Add Custom Provider Enter: Provider name/slug Base URL (e.g., http://localhost:11434/v1 for Ollama) API key (if required) Use the model in agent config: custom-myollama/llama3.3 This works with:\nOllama â€” local models with OpenAI-compatible API LM Studio â€” local model runner vLLM â€” production inference server text-generation-webui â€” with OpenAI extension LocalAI â€” drop-in OpenAI replacement Any OpenAI-compatible API endpoint","extended-thinking#Extended Thinking":"Some models support extended thinking (reasoning tokens). Configure per-agent:\nthinking_level: medium # off, low, medium, high Supported models include Claude Sonnet 4+, Claude Opus 4+, and other reasoning-capable models. The system automatically detects which models support thinking.","global-defaults#Global Defaults":"Set in the dashboard Settings page:\nDefault working model â€” used when no model is specified Default thinking model â€” used for extended reasoning","local-models-via-ollama#Local Models via Ollama":"To use local models:\nInstall Ollama on your host Pull a model: ollama pull llama3.3 Ollama serves at http://localhost:11434/v1 by default Add as a custom provider in DjinnBot settings: Base URL: http://host.docker.internal:11434/v1 (use host.docker.internal from inside Docker) No API key needed Set agent model: custom-ollama/llama3.3 Local models work for chat and simple tasks but may not reliably produce structured output or use tools. For production pipeline execution, cloud models (Claude, GPT-4, Kimi) are recommended.","memory-search-provider#Memory Search Provider":"ClawVaultâ€™s semantic search (QMDR) uses a separate provider for embeddings and reranking. By default, this uses OpenRouter with:\nEmbeddings: openai/text-embedding-3-small Reranking: openai/gpt-4o-mini (LLM-based reranking) Configure via environment variables:\nQMD_OPENAI_API_KEY=${OPENROUTER_API_KEY} QMD_OPENAI_BASE_URL=https://openrouter.ai/api/v1 QMD_EMBED_PROVIDER=openai QMD_OPENAI_EMBED_MODEL=openai/text-embedding-3-small QMD_RERANK_PROVIDER=openai QMD_RERANK_MODE=llm QMD_OPENAI_MODEL=openai/gpt-4o-mini These are set in docker-compose.yml for the engine service. Adjust if you want to use a different embedding provider.","model-selection#Model Selection":"","per-agent#Per-Agent":"Set the default model in agents//config.yml:\nmodel: anthropic/claude-sonnet-4 thinking_model: anthropic/claude-opus-4 thinking_level: medium","per-pipeline-step#Per-Pipeline Step":"Override in pipeline YAML:\nsteps: - id: SPEC agent: eric model: anthropic/claude-opus-4 # Use Opus for requirements - id: IMPLEMENT agent: yukihiro model: openrouter/moonshotai/kimi-k2.5 # Use Kimi for coding","supported-providers#Supported Providers":"","via-dashboard#Via Dashboard":"Go to Settings â†’ Providers Enter API keys for each provider Keys are encrypted at rest using SECRET_ENCRYPTION_KEY Dashboard-configured keys take precedence over .env keys.","via-env-file#Via .env File":"Add API keys to your .env:\nOPENROUTER_API_KEY=sk-or-v1-your-key ANTHROPIC_API_KEY=sk-ant-your-key OPENAI_API_KEY=sk-your-key","why-openrouter-is-recommended#Why OpenRouter is Recommended":"OpenRouter acts as a unified gateway â€” one API key gives you access to every major model. This means:\nTest different models per agent without managing multiple keys Fall back to alternative models if one is down Access the latest models as they launch Single billing for everything"},"title":"LLM Providers"},"/docs/advanced/local-development/":{"data":{"":"For contributing to DjinnBot or running without Docker.","build#Build":"# Build all TypeScript packages npm run build # Build specific package npm run build --filter=@djinnbot/core # Type checking npm run typecheck # Linting npm run lint","database-migrations#Database Migrations":"The API server uses Alembic for PostgreSQL migrations:\ncd packages/server # Check migration status alembic current # Create a new migration alembic revision --autogenerate -m \"add_new_field\" # Run migrations alembic upgrade head Migrations run automatically on API server startup via ensure_migrations().","environment-variables#Environment Variables":"Create a .env file at the repo root:\nOPENROUTER_API_KEY=sk-or-v1-your-key DATABASE_URL=postgresql+asyncpg://djinnbot:djinnbot@localhost:5432/djinnbot REDIS_URL=redis://localhost:6379 PIPELINES_DIR=./pipelines AGENTS_DIR=./agents LOG_LEVEL=DEBUG","install-dependencies#Install Dependencies":"# TypeScript packages (monorepo root) npm install # Python API server cd packages/server pip install -e \".[dev]\" cd ../.. # Python CLI cd cli pip install -e . cd ..","prerequisites#Prerequisites":"Node.js 20+ Python 3.12+ with pip PostgreSQL 16 Redis 7+ Go (for Hugo module support, if building docs) Docker (still needed for agent containers and mcpo)","project-structure#Project Structure":"The monorepo uses Turborepo for build orchestration:\npackage.json # Root workspace config turbo.json # Build pipeline config packages/ â”œâ”€â”€ core/ # Pipeline engine (TypeScript) â”‚ â””â”€â”€ src/ â”‚ â”œâ”€â”€ engine/ # Pipeline state machine â”‚ â”œâ”€â”€ events/ # Redis Streams event bus â”‚ â”œâ”€â”€ runtime/ # Agent executor â”‚ â”œâ”€â”€ container/ # Docker container management â”‚ â”œâ”€â”€ memory/ # ClawVault integration â”‚ â”œâ”€â”€ mcp/ # MCP/mcpo manager â”‚ â”œâ”€â”€ skills/ # Skill registry â”‚ â”œâ”€â”€ chat/ # Chat session manager â”‚ â”œâ”€â”€ projects/ # Project/task management â”‚ â”œâ”€â”€ lifecycle/ # Agent activity tracking â”‚ â””â”€â”€ sessions/ # Session management â”œâ”€â”€ server/ # API server (Python/FastAPI) â”‚ â””â”€â”€ app/ â”‚ â”œâ”€â”€ routers/ # REST endpoints (30+ routers) â”‚ â”œâ”€â”€ models/ # SQLAlchemy models â”‚ â”œâ”€â”€ services/ # Business logic â”‚ â””â”€â”€ alembic/ # Database migrations â”œâ”€â”€ dashboard/ # Web UI (React/TypeScript) â”‚ â””â”€â”€ src/ â”‚ â”œâ”€â”€ routes/ # TanStack Router pages â”‚ â”œâ”€â”€ components/ # React components â”‚ â”œâ”€â”€ hooks/ # Custom React hooks â”‚ â””â”€â”€ lib/ # API client, SSE, formatters â”œâ”€â”€ slack/ # Slack integration (TypeScript) â”‚ â””â”€â”€ src/ â”‚ â”œâ”€â”€ slack-bridge.ts â”‚ â”œâ”€â”€ agent-slack-runtime.ts â”‚ â””â”€â”€ thread-manager.ts â””â”€â”€ agent-runtime/ # Container entrypoint (TypeScript) â””â”€â”€ src/ â”œâ”€â”€ entrypoint.ts â”œâ”€â”€ agent/ # Agent initialization â””â”€â”€ tools/ # Container-side tools (bash, read, write, edit)","setup#Setup":"","start-infrastructure#Start Infrastructure":"You need PostgreSQL and Redis running. The simplest way is Docker for just those services:\ndocker compose up -d postgres redis Or run them locally:\n# macOS brew services start postgresql@16 brew services start redis","start-services#Start Services":"Run each service in a separate terminal:\n# 1. API Server cd packages/server uvicorn app.main:app --reload --port 8000 # 2. Pipeline Engine cd packages/core npm run build node dist/main.js # 3. Dashboard (dev server with hot reload) cd packages/dashboard npm run dev"},"title":"Local Development"},"/docs/advanced/security/":{"data":{"":"DjinnBot takes security seriously with container isolation, encrypted secrets, and minimal attack surface.","comparison-with-other-tools#Comparison with Other Tools":"Unlike tools that execute agent code directly on the host:\nDjinnBot agents cannot access your files outside their workspace DjinnBot agents cannot read your SSH keys, browser cookies, or environment DjinnBot agents cannot install packages on your system DjinnBot agents cannot run as root on your machine The container boundary is a hard security line. The worst case scenario is an agent doing damage inside its own ephemeral container, which is destroyed after the step completes.","container-isolation#Container Isolation":"Every agent runs in its own Docker container. This provides:\nFilesystem isolation â€” agents cannot access the host filesystem Process isolation â€” agents cannot see or interact with host processes Network isolation â€” containers are on a private bridge network No Docker socket â€” agent containers cannot spawn other containers Ephemeral execution â€” containers are destroyed after each step, leaving no persistent state outside the data volume The engine container has Docker socket access (required to spawn agent containers), but this is limited to the engine service only. Agent containers receive no Docker socket access.","credential-injection#Credential Injection":"Provider API keys are injected into agent containers as environment variables â€” theyâ€™re never baked into images or written to disk. The engine fetches keys from the database and passes them to containers at runtime.","default-configuration#Default Configuration":"The default Docker Compose setup exposes services on localhost:\nService Bound To Port Dashboard 0.0.0.0:3000 3000 API 0.0.0.0:8000 8000 mcpo 0.0.0.0:8001 8001 PostgreSQL 0.0.0.0:5432 5432 Redis 0.0.0.0:6379 6379","encryption-at-rest#Encryption at Rest":"User-defined secrets (API keys, SSH keys, tokens) are encrypted with AES-256-GCM before storage in PostgreSQL. The encryption key is configured via SECRET_ENCRYPTION_KEY in .env.\n# Generate a strong encryption key python3 -c \"import secrets; print(secrets.token_hex(32))\" Without SECRET_ENCRYPTION_KEY, secrets are encrypted with an ephemeral key that changes on restart â€” making stored secrets permanently unrecoverable. Always set this in production.","internal-network#Internal Network":"All DjinnBot services communicate on the djinnbot_default Docker bridge network. Agent containers are attached to this network for Redis and API access, but cannot reach the host network.","mcp-proxy-authentication#MCP Proxy Authentication":"The mcpo proxy is protected by MCPO_API_KEY. Agent containers receive this key to authenticate tool calls. Generate a strong key:\npython3 -c \"import secrets; print(secrets.token_urlsafe(32))\"","network-security#Network Security":"","production-hardening#Production Hardening":"For production deployments:\nBind to localhost â€” change port bindings to 127.0.0.1:PORT:PORT Reverse proxy â€” put nginx or Caddy in front of the API and dashboard with TLS Firewall â€” block external access to PostgreSQL, Redis, and mcpo ports Change defaults â€” update PostgreSQL password, mcpo API key, and encryption key","secrets-management#Secrets Management":""},"title":"Security Model"},"/docs/concepts/":{"data":{"":"Understand how DjinnBot works under the hood. These concepts form the foundation of everything the platform does.\nArchitectureHow the services, event bus, and containers fit together. AgentsPersonas, memory, tools, and the files that define an agent. PipelinesYAML workflow definitions with steps, loops, and branching. Memory SystemClawVault, semantic search, wiki-links, and knowledge graphs. SkillsOn-demand instruction sets agents load when they need them. MCP ToolsExternal tool servers via the mcpo proxy. Pulse ModeAutonomous agent wake-up cycles for ongoing work. Agent ContainersIsolated Docker environments with full toolboxes."},"title":"Core Concepts"},"/docs/concepts/agents/":{"data":{"":"Agents are the core of DjinnBot. Each agent is a specialized AI persona with its own identity, expertise, memory, and tools. Theyâ€™re not generic LLM wrappers â€” theyâ€™re characters with opinions and domain knowledge.","agent-files#Agent Files":"Every agent is defined by a directory under agents/:\nagents/eric/ â”œâ”€â”€ IDENTITY.md # Name, origin, role, emoji â”œâ”€â”€ SOUL.md # Deep personality, beliefs, anti-patterns, voice â”œâ”€â”€ AGENTS.md # Workflow procedures, collaboration triggers, tool usage â”œâ”€â”€ DECISION.md # Memory-first decision framework â”œâ”€â”€ PULSE.md # Autonomous wake-up routine â”œâ”€â”€ config.yml # Model, pulse schedule, thinking settings â””â”€â”€ slack.yml # Slack bot credentials (optional)","agent-templates#Agent Templates":"Shared templates in agents/_templates/ provide common workflow and memory instructions that all agents inherit:\nAGENTS.md â€” environment description, git workflow, memory tools, communication tools DECISION.md â€” memory-first decision framework PULSE.md â€” autonomous wake-up routine with project tools MEMORY_TOOLS.md â€” detailed memory tool reference with examples When creating a new agent, these templates provide the baseline behavior. Agent-specific files add role-specific expertise on top.","agentsmd#AGENTS.md":"The workflow file. This tells the agent exactly how to do their job:\nSession startup â€” what to do every time they wake up (read SOUL, search memories) Step-by-step procedures â€” detailed workflows for their role Collaboration triggers â€” when to loop in other agents Tool usage â€” how to use memory, research, messaging, and domain tools Templates â€” output formats and document structures","business-agents#Business Agents":"Agent Role Expertise Holt Marketing \u0026 Sales Sales strategy, outreach, deal management Luke SEO Specialist Content strategy, keyword research, technical SEO Jim Finance Lead Budget, pricing, runway, financial modeling Business agents currently work in chat and pulse modes. Structured marketing/sales pipeline support is on the roadmap.","configyml#config.yml":"Runtime configuration for the agent:\nmodel: xai/grok-4-1-fast-reasoning # Default LLM model thinking_model: xai/grok-4-1-fast-reasoning # Model for thinking/reasoning thinking_level: 'off' # off, low, medium, high thread_mode: passive # passive or active Slack mode pulse_enabled: false # Autonomous pulse mode pulse_interval_minutes: 30 # How often to wake up pulse_columns: # Which kanban columns to check - Backlog - Ready pulse_container_timeout_ms: 120000 # Max container runtime pulse_blackouts: # Don't pulse during these times - label: Nighttime start_time: '23:00' end_time: '07:00' type: recurring All configuration can be edited through the dashboard Settings page.","creating-custom-agents#Creating Custom Agents":"To create a new agent:\nCreate a directory under agents/ with the agentâ€™s ID Add at minimum IDENTITY.md, SOUL.md, and config.yml Copy AGENTS.md, DECISION.md, and PULSE.md from agents/_templates/ and customize Restart the engine to pick up the new agent The agent will immediately be available in the dashboard for chat sessions and can be referenced in pipeline YAML files.","decisionmd#DECISION.md":"A memory-first decision framework shared across agents:\nSearch memories before every response Create memories when learning something new Reflect on interactions for self-improvement Stay in character and add value","engineering-pipeline-agents#Engineering Pipeline Agents":"Agent Role Pipeline Stages Expertise Eric Product Owner SPEC Requirements, user stories, scope, prioritization Finn Solutions Architect DESIGN, REVIEW Architecture, tech decisions, code review Shigeo UX Specialist UX User flows, design systems, accessibility Yukihiro Senior SWE IMPLEMENT, FIX Writing code, debugging, implementation Chieko Test Engineer TEST QA, test strategy, regression detection Stas SRE DEPLOY Infrastructure, deployment, monitoring Yang DevEx Specialist DX (on-demand) CI/CD, tooling, developer workflow","how-agents-execute#How Agents Execute":"When an agent is assigned a pipeline step or chat message:\nThe engine spawns a Docker container from Dockerfile.agent-runtime The container loads the agentâ€™s persona files (IDENTITY + SOUL + AGENTS + DECISION) ClawVault memories are loaded and injected as context Skills matching the task keywords are auto-injected The agent receives the step input (with template variables resolved) The agent works â€” calling tools, writing files, running commands Output streams back to the engine via Redis pub/sub Memories are saved on session end The container is destroyed Each execution is stateless at the container level â€” all persistence comes from the database, memory vaults, and git workspaces.","identitymd#IDENTITY.md":"The basics â€” name, origin country, role title, emoji, and which pipeline stage(s) this agent handles.\n# Eric â€” Product Owner - **Name:** Eric - **Origin:** Denmark - **Role:** Product Owner - **Abbreviation:** PO - **Emoji:** ğŸ“‹ - **Pipeline Stage:** SPEC","pulsemd#PULSE.md":"The autonomous wake-up routine for pulse mode. See Pulse Mode for details.","slackyml#slack.yml":"Slack bot credentials for this agent (see Slack Setup):\nbot_token: ${SLACK_ERIC_BOT_TOKEN} app_token: ${SLACK_ERIC_APP_TOKEN}","soulmd#SOUL.md":"The personality file. This is what makes agents feel real. It includes:\nWho they are â€” backstory, experience, what shaped their approach Core beliefs â€” principles forged through experience (e.g., â€œvague specs produce vague resultsâ€) Anti-patterns â€” things they refuse to do, with reasoning Productive flaws â€” intentional trade-offs (e.g., Eric is ruthlessly aggressive about cutting scope) How they work â€” their process for their domain Collaboration style â€” how they interact with other agents Key phrases â€” characteristic things they say The SOUL file is typically 100-200 lines of rich, specific character definition. This is injected into the agentâ€™s system prompt.","the-default-team#The Default Team":"DjinnBot ships with 10 agents covering a full product organization:"},"title":"Agents"},"/docs/concepts/architecture/":{"data":{"":"DjinnBot is a distributed system built around an event-driven architecture. Hereâ€™s how the pieces fit together.","agent-containers#Agent Containers":"Each agent step spawns a fresh Docker container built from Dockerfile.agent-runtime. See Agent Containers for details.","api-server-python--fastapi#API Server (Python / FastAPI)":"The API server is the central coordination point. It provides:\nREST API â€” all CRUD operations for runs, pipelines, agents, projects, settings, memory, etc. SSE streaming â€” Server-Sent Events for real-time dashboard updates Database access â€” PostgreSQL via SQLAlchemy with Alembic migrations GitHub webhooks â€” receive events from GitHub for issue/PR integration Chat management â€” session lifecycle for interactive agent conversations The API server does not execute agents. It stores state and serves the frontend.","dashboard-react--vite--tanstack-router#Dashboard (React / Vite / TanStack Router)":"A single-page application built with:\nReact with TypeScript TanStack Router for file-based routing Tailwind CSS for styling SSE for real-time streaming updates The dashboard talks directly to the API server â€” thereâ€™s no backend-for-frontend. Itâ€™s served as static files by nginx in the Docker container.","event-flow#Event Flow":"Hereâ€™s what happens when you start a pipeline run:\nDashboard â†’ POST /v1/runs â†’ API Server creates run in PostgreSQL API Server â†’ publishes RUN_CREATED event â†’ Redis Streams Engine picks up event â†’ creates run state machine â†’ publishes STEP_QUEUED for first step Engine â†’ spawns Agent Container with persona, memories, and workspace Agent executes step â†’ streams output chunks via Redis Pub/Sub â†’ Engine relays to API Server â†’ Dashboard displays via SSE Agent completes â†’ Engine evaluates result â†’ routes to next step (or branches, retries, loops) Steps continue until pipeline completes or fails","mcpo-proxy#mcpo Proxy":"The mcpo proxy exposes MCP tool servers as REST/OpenAPI endpoints. See MCP Tools for details.","monorepo-structure#Monorepo Structure":"DjinnBot is a Turborepo monorepo with npm workspaces:\npackages/ â”œâ”€â”€ core/ # Engine, events, memory, container management (TypeScript) â”œâ”€â”€ server/ # API server (Python/FastAPI) â”œâ”€â”€ dashboard/ # Web UI (React/TypeScript) â”œâ”€â”€ slack/ # Slack bridge and per-agent bots (TypeScript) â””â”€â”€ agent-runtime/ # Container entrypoint and tools (TypeScript) The core package contains the bulk of the orchestration logic â€” the pipeline engine, Redis event bus, container runner, ClawVault memory integration, skill registry, MCP manager, chat session manager, and more.","pipeline-engine-typescript--nodejs#Pipeline Engine (TypeScript / Node.js)":"The engine is the brain of the system. It:\nRuns the state machine â€” advances pipeline steps based on events Spawns agent containers â€” creates isolated Docker containers for each agent step Manages memory â€” loads/saves ClawVault memories for each agent session Bridges Slack â€” routes events to Slack threads and processes agent mentions Manages MCP â€” writes tool server config and monitors health via the McpoManager Processes events â€” subscribes to Redis Streams for reliable, ordered event delivery The engine communicates with agent containers via Redis pub/sub â€” sending commands (run step, send message) and receiving events (output chunks, tool calls, completion).","postgresql-state-store#PostgreSQL (State Store)":"All persistent state lives in PostgreSQL:\nPipeline run state and history Step outputs and timing Agent configuration Project boards and tasks Settings and provider configuration MCP server registry Chat session messages Secrets (encrypted at rest)","redis-event-bus#Redis (Event Bus)":"Redis serves two roles:\nStreams â€” reliable, ordered event delivery between the API server and engine. Events like RUN_CREATED, STEP_QUEUED, STEP_COMPLETE flow through Redis Streams. Pub/Sub â€” real-time communication between the engine and agent containers. The engine sends commands, agents publish output chunks and events.","services#Services":"","system-overview#System Overview":"â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ Dashboard â”‚â—„â”€SSEâ”€â”‚ API Server â”‚â—„â”€â”€â”€â”€â”€â”‚ PostgreSQL â”‚ â”‚ (React + Vite) â”‚ â”‚ (FastAPI) â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â–² â–¼ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ Pipeline Engine â”‚â”€â”€â”€â”€â”€â–ºâ”‚ Redis â”‚ â”‚ (State Machine) â”‚ â”‚ (Streams) â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â–¼ â–¼ â–¼ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ Agent â”‚ â”‚ Agent â”‚ â”‚ mcpo Proxy â”‚ â”‚Container â”‚ â”‚Container â”‚ â”‚ (MCP Tools) â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜","tech-stack-summary#Tech Stack Summary":"Component Technology Language API Server FastAPI, SQLAlchemy, Alembic Python Pipeline Engine Custom state machine, Redis Streams TypeScript Dashboard React, TanStack Router, Tailwind TypeScript Agent Runtime pi-mono (pi-agent-core) TypeScript Agent Containers Debian bookworm, full toolbox Multi-language Memory ClawVault + QMDR TypeScript Event Bus Redis Streams + Pub/Sub â€” Database PostgreSQL 16 â€” MCP Proxy mcpo Python Build System Turborepo â€” Orchestration Docker Compose â€”"},"title":"Architecture"},"/docs/concepts/containers/":{"data":{"":"Every agent session runs in an isolated Docker container. This is one of DjinnBotâ€™s key differentiators â€” agents get a full engineering environment without any host access.","container-lifecycle#Container Lifecycle":"Spawn â€” engine creates a new container via Docker API Mount â€” shared data volume is mounted, symlinks are set up for agent home directory Inject â€” persona files, memories, secrets, and environment variables are provided Execute â€” agent runtime starts, processes the step or chat message Stream â€” output flows to engine via Redis pub/sub Destroy â€” container is removed when the step completes Each step gets a fresh container â€” thereâ€™s no state leaking between steps. Persistence comes from:\nData volume â€” shared across containers for memory vaults and workspaces Database â€” PostgreSQL stores step outputs, chat history, etc. Git â€” code changes are committed and pushed","customization#Customization":"To add tools to the agent container, modify Dockerfile.agent-runtime and rebuild:\ndocker compose build engine docker compose up -d The agent-runtime image is built independently from the other services, so you can customize the toolbox without affecting the API server or dashboard.","developer-tools#Developer Tools":"git + git-lfs â€” version control GitHub CLI (gh) â€” PR and issue management ripgrep (rg) â€” fast code search fd â€” fast file finder bat â€” syntax-highlighted cat fzf â€” fuzzy finder delta â€” better git diffs eza â€” modern ls replacement jq + yq â€” JSON/YAML processing tree, dust â€” directory visualization","languages--runtimes#Languages \u0026amp; Runtimes":"Node.js 22 â€” JavaScript/TypeScript Python 3 â€” with pip and venv Go 1.23+ â€” compiled language support Rust â€” via rustup (stable toolchain) Bun 1.3.6 â€” for QMDR (semantic search)","memory-tools#Memory Tools":"QMDR â€” semantic search CLI (ClawVault integration) ClawVault CLI â€” direct memory management","security-model#Security Model":"No host access â€” containers cannot reach the host filesystem Network isolation â€” containers are on the djinnbot_default bridge network No Docker socket â€” agent containers cannot spawn other containers (only the engine can) Ephemeral â€” containers are destroyed after each step Credential injection â€” API keys come from environment variables, not baked into the image The engine container has Docker socket access (needed to spawn agent containers), but agent containers themselves are fully sandboxed.","system-utilities#System Utilities":"curl, wget, httpie â€” HTTP clients netcat, socat, dnsutils â€” network tools imagemagick â€” image processing sqlite3, postgresql-client, redis-tools â€” database clients make, cmake, autoconf â€” build systems vim, nano â€” text editors htop, lsof, strace â€” system monitoring","volume-layout#Volume Layout":"Agent containers mount the shared djinnbot-data volume at /data. The containerâ€™s home directory (/home/agent) is symlinked to /data/sandboxes/{agentId}/, giving each agent a persistent home across sessions.\n/home/agent/ â†’ /data/sandboxes/{agentId}/ â”œâ”€â”€ clawvault/ â”‚ â”œâ”€â”€ {agent-id}/ â† personal memory vault â”‚ â””â”€â”€ shared/ â† team shared knowledge â”œâ”€â”€ run-workspace/ â† git worktree (pipeline sessions) â”œâ”€â”€ project-workspace/ â† full project repo (pipeline sessions) â””â”€â”€ task-workspaces/ â””â”€â”€ {taskId}/ â† authenticated git workspace (pulse sessions)","whats-inside#What\u0026rsquo;s Inside":"The agent container (Dockerfile.agent-runtime) is built on Debian bookworm and includes:"},"title":"Agent Containers"},"/docs/concepts/mcp-tools/":{"data":{"":"DjinnBot agents can use external tools through the Model Context Protocol (MCP). An mcpo proxy converts MCP tool servers into REST/OpenAPI endpoints that agents call like any other tool.","adding-tools#Adding Tools":"","configuration#Configuration":"MCP servers are configured in mcp/config.json:\n{ \"mcpServers\": { \"github\": { \"command\": \"/usr/local/bin/github-mcp-server\", \"args\": [\"stdio\", \"--read-only\", \"--toolsets\", \"context,repos,issues\"], \"env\": { \"GITHUB_PERSONAL_ACCESS_TOKEN\": \"${GITHUB_TOKEN}\" } }, \"time\": { \"command\": \"uvx\", \"args\": [\"mcp-server-time\", \"--local-timezone=Europe/Lisbon\"] } } } Each entry specifies:\ncommand â€” the executable to run args â€” command-line arguments env â€” environment variables (supports ${VAR} substitution from the host)","custom-mcp-servers#Custom MCP Servers":"You can add any MCP-compatible tool server. The community maintains hundreds of servers for various APIs and services. Some popular ones:\nFilesystem â€” file operations outside the workspace Postgres/MySQL â€” direct database access Slack â€” Slack API operations Brave Search â€” web search Playwright â€” browser automation See awesome-mcp-servers for a comprehensive list.","default-tools#Default Tools":"DjinnBot ships with these MCP servers pre-configured:\nServer Tools Description github Repository browsing, issues, context GitHub API access (read-only by default) fetch Web page fetching Retrieve content from URLs time Current time/timezone Time-aware operations grokipedia Wikipedia search Knowledge lookup","health-monitoring#Health Monitoring":"The engineâ€™s McpoManager continuously monitors tool server health:\nOn startup, it polls each server endpoint Server status (running/error/configuring) is tracked in the database Tool discovery extracts available tools from each serverâ€™s OpenAPI schema Logs are streamed to Redis and visible in the dashboard","how-it-works#How It Works":"Agent Container â†’ HTTP â†’ mcpo Proxy â†’ stdio â†’ MCP Server The mcpo proxy runs as a Docker service (djinnbot-mcpo) It reads mcp/config.json to discover configured tool servers Each MCP server is exposed as a set of REST endpoints Agents call these endpoints like any other HTTP tool The proxy handles stdio communication with the actual MCP server","security#Security":"The mcpo proxy is protected by an API key (MCPO_API_KEY in .env). Agent containers receive this key and include it in their requests.\nEnvironment variables in mcp/config.json use ${VAR} syntax â€” the actual values come from the Docker environment, not the config file. Sensitive tokens never appear in the config.","via-api#Via API":"POST /v1/mcp/ { \"name\": \"My Tool Server\", \"description\": \"Does useful things\", \"config\": { \"command\": \"npx\", \"args\": [\"my-mcp-server\"], \"env\": {} }, \"enabled\": true }","via-config-file#Via Config File":"Edit mcp/config.json directly. The proxy watches the file and reloads automatically.","via-dashboard#Via Dashboard":"Go to MCP Tools in the dashboard Click Configure or Add Server Enter the server configuration The proxy hot-reloads â€” no restart needed"},"title":"MCP Tools"},"/docs/concepts/memory/":{"data":{"":"DjinnBot agents have persistent memory that survives across sessions. They remember decisions, learn from mistakes, and build knowledge over time. This is powered by ClawVault with semantic search via QMDR.","browsing-memory#Browsing Memory":"You can browse and search agent memories through:\nDashboard â€” the Memory page lets you view vaults and search semantically CLI â€” djinnbot memory search eric \"architecture decisions\" API â€” GET /v1/memory/search?agent_id=eric\u0026query=architecture","how-it-works#How It Works":"Every agent has two memory vaults:\nPersonal vault (data/vaults//) â€” private memories only this agent can access Shared vault (data/vaults/shared/) â€” team-wide knowledge all agents can read and write Memories are stored as markdown entries with metadata (type, tags, timestamps) and connected via wiki-links for graph traversal.","memory-lifecycle#Memory Lifecycle":"Wake â€” when an agent session starts, ClawVault loads relevant memories into context Recall â€” during execution, agents search memories semantically before making decisions Remember â€” agents save important findings, decisions, and lessons Checkpoint â€” during long sessions, memories are periodically saved Sleep â€” on session end, a summary is saved with next steps","memory-tools#Memory Tools":"","recall--search-memories#recall â€” Search Memories":"recall(\"search query\", { limit: 5, profile: \"default\" }) Profiles optimize retrieval for different contexts:\nProfile Use Case default General purpose retrieval planning Task planning and project context incident Errors, bugs, and lessons learned handoff Session continuity information","remember--save-to-vault#remember â€” Save to Vault":"remember(type, \"Title\", \"Content with details\", { shared: true, // Share with all agents tags: [\"tag1\"] // For search filtering }) Memory types:\nType When to Use lesson Learned from a mistake or success decision Important choice with rationale pattern Recurring approach that works fact Important information about the project/team preference How someone or something prefers to work handoff Context for resuming work later","semantic-search#Semantic Search":"Memory search uses embeddings for semantic similarity, not just keyword matching. When an agent calls recall(\"how we handle authentication\"), it finds memories about auth patterns even if they donâ€™t contain the exact word â€œauthentication.â€\nThe search pipeline:\nQuery expansion â€” the query is expanded to capture related concepts Embedding â€” query is converted to a vector via text-embedding-3-small Retrieval â€” nearest neighbors found in the SQLite-backed vector store Reranking â€” results are reranked using gpt-4o-mini for relevance Injection â€” top results are injected into the agentâ€™s context All embedding and reranking runs through OpenRouter â€” no local GPU or model downloads required.","the-anchor-pattern#The Anchor Pattern":"Every project should have a root anchor memory that links to all related knowledge:\nremember(\"fact\", \"Project: MyApp\", \"Root anchor for project MyApp.\\n\" + \"Goal: [[MyApp: Goal]]\\n\" + \"Tech: [[MyApp: Tech Stack]]\\n\" + \"Scope: [[MyApp: V1 Scope]]\", { shared: true, tags: [\"project:myapp\", \"project-anchor\"] } ) Subsequent memories link back to the anchor, keeping the graph connected.","wiki-link-knowledge-graph#Wiki-Link Knowledge Graph":"Memories are connected using [[wiki-link]] syntax:\nremember(\"decision\", \"MyApp: Tech Stack\", \"[[Project: MyApp]] will use FastAPI + PostgreSQL. \" + \"Considered Django (rejected: too opinionated) and Express (rejected: need Python). \" + \"See also [[MyApp: Architecture]], [[MyApp: API Design]].\", { shared: true, tags: [\"project:myapp\", \"architecture\"] } ) These links create a traversable knowledge graph. When an agent recalls context about a project, they start from [[Project: Name]] and follow links to discover related information."},"title":"Memory System"},"/docs/concepts/pipelines/":{"data":{"":"Pipelines are YAML files that define multi-agent workflows. Each pipeline describes a series of steps, which agents handle them, and how results flow between steps.","bugfix#bugfix":"For diagnosing and fixing bugs:\nFIX â†’ VALIDATE 2 agents: Yukihiro (diagnose + fix), Chieko (validate).","built-in-pipelines#Built-in Pipelines":"","creating-custom-pipelines#Creating Custom Pipelines":"Create a YAML file in pipelines/ and it will automatically appear in the dashboard. No restart needed â€” the API reads pipeline definitions from disk.\nA minimal custom pipeline:\nid: my-pipeline name: My Custom Pipeline version: 1.0.0 description: Does something useful defaults: model: anthropic/claude-sonnet-4 tools: [read, write, bash] timeout: 600 agents: - id: yukihiro name: Yukihiro (SWE) steps: - id: DO_THING agent: yukihiro input: | Task: {{task_description}} Do the thing. outputs: [result] You can reference any agent defined in agents/ and use any combination of steps, branches, and loops.","engineering#engineering":"The full software development lifecycle:\nSPEC â†’ DESIGN â†’ UX â†’ IMPLEMENT (loop) â†” REVIEW â†” TEST â†’ DEPLOY All 7 engineering agents participate. The IMPLEMENT step loops over a task breakdown, with each implementation reviewed and tested before moving on.","execute#execute":"Runs a single task from a project board in an isolated container.","feature#feature":"A lighter pipeline for adding features to existing code:\nDESIGN â†’ IMPLEMENT â†” REVIEW â†’ TEST 3 agents: Finn (design + review), Yukihiro (implement), Chieko (test).","loop-steps#Loop Steps":"Steps can iterate over a list produced by a previous step:\n- id: IMPLEMENT agent: yukihiro input: | Current Task: {{current_item}} Completed Tasks: {{completed_items}} loop: over: task_breakdown_json # Iterate over this output onEachComplete: REVIEW # After each item, go here onAllComplete: DEPLOY # After all items, go here The engine parses the JSON array and executes the step once per item, injecting {{current_item}}, {{completed_items}}, and {{progress_file}} template variables.","pipeline-structure#Pipeline Structure":"Every pipeline YAML has four sections:\n# Metadata id: engineering name: Engineering Pipeline version: 1.0.0 description: Full software development workflow # Defaults applied to all steps defaults: model: openrouter/moonshotai/kimi-k2.5 tools: [read, write, bash] maxRetries: 3 timeout: 4800 # Agent declarations agents: - id: eric name: Eric (Product Owner) persona: docs/personas/eric.md tools: [web_search, read, write] # Step definitions steps: - id: SPEC agent: eric input: | You are the Product Owner. Task: {{task_description}} ... outputs: [product_brief, requirements_doc] onComplete: DESIGN","planning#planning":"Decomposes a project into tasks with dependency chains:\nDECOMPOSE â†’ VALIDATE â†’ DECOMPOSE_SUBTASKS â†’ VALIDATE_SUBTASKS Uses structured output mode â€” no tools, just constrained JSON. Eric breaks down the project, Finn validates and enriches. Then Eric creates bite-sized subtasks, and Finn validates those too. The output integrates directly with the project board.","result-routing#Result Routing":"Steps can branch based on agent output using onResult:\n- id: REVIEW agent: finn outputs: [review_result, review_feedback] onResult: APPROVED: goto: TEST CHANGES_REQUESTED: goto: IMPLEMENT The agent writes a result value (e.g., APPROVED to REVIEW_RESULT.txt), and the engine routes to the matching branch.","steps#Steps":"Each step defines:\nField Description id Unique identifier (e.g., SPEC, DESIGN, IMPLEMENT) agent Which agent handles this step input The prompt template sent to the agent outputs Named outputs the agent produces onComplete Next step when this one succeeds onResult Conditional routing based on agent output loop Execute over a list of items model Override the default model for this step timeout Override the default timeout outputSchema JSON schema for structured output (no tools mode)","structured-output#Structured Output":"Steps can enforce structured JSON output using outputSchema:\n- id: DECOMPOSE agent: eric outputSchema: name: task_breakdown strict: true schema: type: object properties: tasks: type: array items: type: object properties: title: { type: string } priority: { type: string, enum: [P0, P1, P2, P3] } estimatedHours: { type: number } required: [title, priority, estimatedHours] required: [tasks] When outputSchema is specified, the step uses direct API calls with constrained JSON output instead of tool-based execution. This guarantees valid, parseable output.","template-variables#Template Variables":"Step inputs can reference outputs from previous steps using {{variable_name}}:\n- id: DESIGN agent: finn input: | Requirements: {{requirements_doc}} User Stories: {{user_stories_json}} Jinja2 conditionals are also supported:\ninput: | {% if review_feedback %} Review Feedback: {{review_feedback}} {% endif %}"},"title":"Pipelines"},"/docs/concepts/pulse/":{"data":{"":"Pulse mode lets agents work autonomously on a schedule â€” checking their inbox, finding tasks, and doing work without human intervention.","blackout-windows#Blackout Windows":"You can prevent agents from pulsing during certain times:\npulse_blackouts: - label: Nighttime start_time: '23:00' end_time: '07:00' type: recurring - label: Weekend start_time: '2026-03-01T00:00:00' end_time: '2026-03-03T00:00:00' type: one_off","communication#Communication":"Agents can communicate during pulse sessions:\nmessage_agent(agentId, message) â€” send a message to another agentâ€™s inbox slack_dm(message) â€” message the human via Slack DM (use sparingly â€” only for urgent findings or blockers)","configuration#Configuration":"Pulse is configured per-agent in config.yml:\npulse_enabled: true pulse_interval_minutes: 30 pulse_columns: # Which kanban columns this agent checks - Backlog - Ready pulse_container_timeout_ms: 120000 pulse_max_consecutive_skips: 5 pulse_offset_minutes: 3 # Stagger to avoid all agents waking simultaneously pulse_blackouts: - label: Nighttime start_time: '23:00' end_time: '07:00' type: recurring pulse_transitions_to: # Allowed kanban transitions - planning - ready - in_progress All settings can be edited through the dashboard agent configuration page.","git-workflow#Git Workflow":"When an agent claims a task, the system:\nCreates a feature branch: feat/task_abc123-implement-oauth Provisions a git workspace at /home/agent/task-workspaces/{taskId}/ Configures git credentials for push access The agent works in this workspace, commits, pushes, and opens a PR â€” all within the isolated container.","how-pulse-works#How Pulse Works":"When pulse mode is enabled for an agent, the engine wakes them up on a configurable interval (default: every 30 minutes). Each pulse cycle follows a routine:\nCheck inbox â€” read messages from other agents Search memories â€” recall recent context, handoffs, and active work Discover projects â€” call get_my_projects() to find assigned projects Check work queue â€” call get_ready_tasks(projectId) to find tasks in their columns Claim a task â€” claim_task(projectId, taskId) atomically assigns the task and provisions a git workspace Do the work â€” implement the task in the provisioned workspace Open a PR â€” open_pull_request(projectId, taskId, title, body) when ready Transition the task â€” move it through the kanban board (e.g., to â€œreviewâ€) Report â€” message the human via Slack DM if thereâ€™s something important Agents only pick up one task per pulse to stay focused.","project-tools#Project Tools":"During pulse sessions, agents have access to project management tools:\nTool Purpose get_my_projects() List assigned projects get_ready_tasks(projectId) Find tasks in your columns claim_task(projectId, taskId) Claim a task + provision git workspace get_task_context(projectId, taskId) Full task details and acceptance criteria open_pull_request(projectId, taskId, title, body) Open a GitHub PR transition_task(projectId, taskId, status) Move task through kanban execute_task(projectId, taskId) Kick off a pipeline run for the task"},"title":"Pulse Mode"},"/docs/concepts/skills/":{"data":{"":"Skills are on-demand instruction sets that agents can load when they need specialized knowledge. Instead of stuffing every possible instruction into an agentâ€™s system prompt, skills are discovered and loaded contextually.","api#API":"# List skills GET /v1/skills # Get a specific skill GET /v1/skills/{name}?agent_id=yukihiro # Create a skill POST /v1/skills { \"name\": \"my-skill\", \"description\": \"Does something useful\", \"tags\": [\"relevant\", \"tags\"], \"content\": \"# My Skill\\n\\n...\", \"scope\": \"global\" } # Toggle enabled state PATCH /v1/skills/{name}/enabled { \"enabled\": false }","automatic-matching#Automatic Matching":"The engine automatically matches skills to pipeline steps by comparing skill tags against the step input text. If a step mentions â€œgithubâ€ or â€œpull request,â€ skills tagged with those keywords are auto-injected.\nThis means agents often get the right skills without explicitly calling load_skill().","creating-effective-skills#Creating Effective Skills":"Good skills are:\nSpecific â€” focused on one domain or task type Procedural â€” step-by-step instructions, not vague guidance Tagged well â€” keywords that match when agents encounter relevant tasks Example-rich â€” show concrete examples of commands, code, and output","dashboard#Dashboard":"The Skills page in the dashboard lets you:\nView all global and agent-specific skills Enable/disable skills without deleting them Edit skill content Create new skills","how-skills-work#How Skills Work":"On session start, the agent receives a skill manifest â€” a compact list of available skills with descriptions When the agent encounters a task that matches a skill, it calls load_skill(\"name\") The full skill instructions are loaded into context The agent follows the skillâ€™s procedures This keeps system prompts lean while giving agents access to deep, specialized knowledge when needed.","managing-skills#Managing Skills":"","skill-file-format#Skill File Format":"Skills are markdown files with YAML frontmatter:\n--- name: github-pr description: Opening and merging GitHub pull requests tags: [github, git, pr, pull-request, merge] enabled: true --- # GitHub PR Skill ## When to Use When you need to open a pull request, review PR changes, or merge branches. ## Steps 1. Ensure all changes are committed and pushed 2. Create PR with descriptive title and body 3. Link to relevant issue or task ...","skill-generator#Skill Generator":"The dashboard includes a skill generator â€” an AI-assisted chat session that helps you create new skills through conversation. Describe what the skill should cover and it writes the instructions.","skill-locations#Skill Locations":"Skills can be stored at two levels:\nLocation Scope Path Global Available to all agents agents/_skills/*.md Agent-specific Only for one agent agents//skills/*.md Agent-specific skills override global skills with the same name."},"title":"Skills"},"/docs/getting-started/":{"data":{"":"Get DjinnBot running in under 5 minutes. By the end of this section youâ€™ll have a full AI development team ready to take on tasks.\nInstallationClone, configure, and start all services with Docker Compose. Your First RunStart an engineering pipeline and watch agents collaborate. Dashboard TourNavigate the real-time dashboard, chat, and project boards."},"title":"Getting Started"},"/docs/getting-started/dashboard-tour/":{"data":{"":"The DjinnBot dashboard at http://localhost:3000 gives you real-time visibility into everything your AI team is doing.","agents#Agents":"The agents page shows all configured agents with:\nStatus â€” current activity (idle, in pipeline, in chat) Configuration â€” model, thinking level, pulse settings Recent runs â€” pipeline steps this agent has executed Memory â€” browse and search the agentâ€™s vault Click an agent to edit their configuration â€” change their default model, enable/disable pulse mode, adjust thinking settings.","chat#Chat":"The chat interface lets you talk directly to any agent. Each chat session:\nSpawns an isolated container with the agentâ€™s full toolbox Loads the agentâ€™s persona and memories Supports multi-turn conversation Allows code execution, file operations, and web research Choose an agent and model from the header, then start typing.","home#Home":"The home page shows:\nSystem status â€” Redis connection, active runs, agent count Recent runs â€” latest pipeline executions with status Active sessions â€” live agent containers currently running Quick actions â€” start a new run, open chat, view projects","mcp-tools#MCP Tools":"Configure MCP tool servers that agents can use:\nServer list â€” view all configured tool servers with status Health monitoring â€” live status (running, error, configuring) Tool discovery â€” see which tools each server provides Add servers â€” configure new MCP servers through the UI Hot reload â€” changes take effect immediately, no restart needed","memory#Memory":"Browse and search agent memory vaults:\nPersonal vaults â€” each agentâ€™s private memories Shared vault â€” team-wide knowledge Semantic search â€” find memories by meaning, not just keywords","pipelines#Pipelines":"Browse available pipeline definitions. View the YAML, see the step graph, and start new runs.","projects#Projects":"The projects page provides kanban-style project management:\nBoard view â€” drag tasks between columns (Backlog, Ready, In Progress, Review, Done) Task details â€” description, acceptance criteria, assigned agent, linked PRs Planning pipeline â€” run the planning pipeline to auto-decompose projects into tasks Pulse integration â€” agents autonomously pick up and work on Ready tasks during pulse cycles","runs#Runs":"The runs page lists all pipeline executions. Click any run to see:\nPipeline visualization â€” step-by-step progress through the pipeline Streaming output â€” real-time text output from the active agent Thinking blocks â€” expandable reasoning sections (for models that support it) Tool calls â€” every tool invocation with arguments and results Step history â€” completed steps with full output and timing","settings#Settings":"Configure global settings:\nLLM providers â€” add API keys for Anthropic, OpenAI, OpenRouter, and other providers Default models â€” set the default working model, thinking model, and Slack decision model Pulse settings â€” enable/disable autonomous pulse mode, set intervals Secrets â€” manage encrypted secrets (GitHub tokens, SSH keys, etc.)","skills#Skills":"Manage agent skills â€” reusable instruction sets that agents can load on demand:\nGlobal skills â€” available to all agents (stored in agents/_skills/) Agent-specific skills â€” scoped to individual agents Enable/disable â€” toggle skills without deleting them Skill generator â€” AI-assisted skill creation via chat"},"title":"Dashboard Tour"},"/docs/getting-started/first-run/":{"data":{"":"Now that DjinnBot is running, letâ€™s kick off a pipeline and watch agents collaborate in real-time.","chat-with-agents#Chat With Agents":"Donâ€™t want to run a pipeline? Talk to any agent directly:\nGo to the Chat page in the dashboard Select an agent (e.g., Finn for architecture advice) Start a conversation Chat sessions use the same isolated containers and full toolbox. Agents can read and write code, search the web, and use all their tools â€” just like in a pipeline, but interactive.","next-steps#Next Steps":"Dashboard TourLearn to navigate the full dashboard interface. Understanding PipelinesLearn how YAML pipelines work. Set Up SlackGive each agent its own Slack bot.","start-a-pipeline-via-api#Start a Pipeline via API":"curl -X POST http://localhost:8000/v1/runs \\ -H \"Content-Type: application/json\" \\ -d '{ \"pipeline_id\": \"engineering\", \"task_description\": \"Build a REST API for a bookmarks manager\" }'","start-a-pipeline-via-cli#Start a Pipeline via CLI":"If you prefer the command line, install the CLI:\ncd cli pip install -e . Then run:\n# List available pipelines djinnbot pipeline list # Start a run djinnbot pipeline start engineering \\ --task \"Build a REST API for a bookmarks manager\" # Watch the output stream djinnbot run stream","start-a-pipeline-via-the-dashboard#Start a Pipeline via the Dashboard":"Open http://localhost:3000\nClick New Run in the top navigation\nSelect the engineering pipeline\nEnter a task description, for example:\nBuild a REST API for a bookmarks manager with CRUD endpoints, tag support, and search. Use FastAPI and SQLite.\nClick Start Run","try-other-pipelines#Try Other Pipelines":"Pipeline Best For Agents Involved engineering Full projects from scratch All engineering agents feature Adding a feature to existing code Finn, Yukihiro, Chieko bugfix Diagnosing and fixing bugs Yukihiro, Chieko planning Breaking down a project into tasks Eric, Finn","watch-in-real-time#Watch in Real-Time":"The dashboard shows:\nPipeline progress â€” which step is active, which are complete Streaming output â€” agent responses stream in real-time as they think and work Thinking blocks â€” expandable sections showing the agentâ€™s reasoning process Tool calls â€” every file read, write, bash command, and git operation Step transitions â€” when agents hand off to the next step","what-happens-next#What Happens Next":"The engineering pipeline assigns work through these stages:\nSPEC (Eric) â†’ DESIGN (Finn) â†’ UX (Shigeo) â†’ IMPLEMENT (Yukihiro) â†• REVIEW (Finn) â†• TEST (Chieko) â†“ DEPLOY (Stas) Eric (Product Owner) reads your task description and produces requirements, user stories, and acceptance criteria Finn (Architect) takes Ericâ€™s requirements and designs the architecture, API, database schema, and task breakdown Shigeo (UX) creates UX specifications and design system guidelines Yukihiro (SWE) implements each task from the breakdown â€” writing actual code in an isolated container Finn reviews each implementation, approving or requesting changes Chieko (QA) tests approved implementations, passing or sending back for fixes Stas (SRE) handles deployment once all tasks pass Each agent runs in its own Docker container with a full toolbox â€” they can read files, write code, run bash commands, use git, and more."},"title":"Your First Run"},"/docs/getting-started/installation/":{"data":{"":"","clone--configure#Clone \u0026amp; Configure":"git clone https://github.com/BaseDatum/djinnbot.git cd djinnbot cp .env.example .env Open .env in your editor and set your API key:\n# Required â€” this is the only thing you must set OPENROUTER_API_KEY=sk-or-v1-your-key-here OpenRouter gives you access to Claude, GPT-4, Gemini, Kimi, and dozens of other models through a single API key. Itâ€™s the fastest way to get started. You can also use direct provider keys (Anthropic, OpenAI, etc.) â€” see LLM Providers for details.","next-steps#Next Steps":"Run Your First PipelineStart an engineering pipeline and watch agents collaborate.","optional-encryption-key#Optional: Encryption Key":"For production deployments, generate a secrets encryption key:\n# Generate and add to .env python3 -c \"import secrets; print('SECRET_ENCRYPTION_KEY=' + secrets.token_hex(32))\" \u003e\u003e .env This encrypts user-defined secrets (API keys, SSH keys, etc.) at rest. Without it, secrets are encrypted with an ephemeral key that resets on restart.","prerequisites#Prerequisites":"You need exactly two things:\nDocker + Docker Compose â€” Install Docker Desktop (includes Compose) An LLM API key â€” OpenRouter is recommended (one key, access to all models) Thatâ€™s it. No Node.js, no Python, no database setup. Docker handles everything.","start-services#Start Services":"docker compose up -d This starts 6 services:\nService Container Port Purpose PostgreSQL djinnbot-postgres 5432 State database Redis djinnbot-redis 6379 Event bus (Redis Streams) API Server djinnbot-api 8000 REST API (FastAPI) Pipeline Engine djinnbot-engine â€” Orchestrates agent execution Dashboard djinnbot-dashboard 3000 React web interface MCP Proxy djinnbot-mcpo 8001 Tool server proxy Check that everything is healthy:\ndocker compose ps You should see all services running with healthy status.","verify#Verify":"Open the dashboard:\nhttp://localhost:3000 Check the API:\ncurl http://localhost:8000/v1/status You should see a JSON response with \"status\": \"ok\" and connected service counts.","what-just-happened#What Just Happened":"Docker Compose built and started the entire stack:\nPostgreSQL stores pipeline runs, steps, agent state, project boards, and settings Redis provides the event bus via Redis Streams â€” reliable, ordered message delivery between services API Server (FastAPI/Python) exposes REST endpoints for the dashboard, CLI, and external integrations Pipeline Engine (TypeScript/Node) runs the state machine that coordinates agent execution, spawns agent containers, manages memory, and bridges Slack Dashboard (React/Vite) serves the web interface with real-time SSE streaming mcpo proxies MCP tool servers (GitHub, web fetch, etc.) as OpenAPI endpoints for agents When a pipeline runs, the engine dynamically spawns agent containers â€” isolated Docker containers with a full engineering toolbox â€” for each step. These are separate from the 6 core services and are created/destroyed per step."},"title":"Installation"},"/docs/guides/":{"data":{"":"Step-by-step walkthroughs for common tasks.\nSlack Bot SetupGive each agent its own Slack bot with AI features. Custom AgentsCreate agents with your own personas and expertise. Custom PipelinesBuild YAML workflows for any multi-step process. Project ManagementUse kanban boards and planning pipelines to manage work. GitHub IntegrationConnect repos, trigger pipelines from issues, and manage PRs."},"title":"Guides"},"/docs/guides/custom-agents/":{"data":{"":"Create your own agents with custom personas, expertise, and behavior. This guide walks through creating a new agent from scratch.","1-identitymd#1. IDENTITY.md":"# Nova â€” Data Engineer - **Name:** Nova - **Origin:** Brazil - **Role:** Data Engineer - **Abbreviation:** DE - **Emoji:** ğŸ“Š - **Pipeline Stage:** DATA","2-soulmd#2. SOUL.md":"This is where you define the character. Write in first person. Be specific about beliefs, experience, and working style. The more detailed, the more consistent the agentâ€™s behavior.\n# Nova â€” Data Engineer ## Who I Am I've spent eight years building data pipelines that actually work in production. Not toy demos, not notebook experiments â€” real systems that process millions of events daily without breaking at 3am. ## Core Beliefs ### On Data Quality Bad data in, bad decisions out. I validate at every boundary. I've seen dashboards that executives trusted show wrong numbers because nobody checked for null values upstream. ### On Simplicity The best pipeline is the one with the fewest moving parts. I've seen teams build Rube Goldberg machines with 15 services when a well-designed SQL pipeline would have done the job. ## Anti-Patterns ### I Will Not Ship Without Tests I've had pipelines silently drop 30% of events because a schema changed upstream and nobody tested for it. ## How I Work ...","3-configyml#3. config.yml":"model: anthropic/claude-sonnet-4 thinking_model: anthropic/claude-sonnet-4 thinking_level: 'off' thread_mode: passive pulse_enabled: false pulse_interval_minutes: 30 pulse_columns: - Ready pulse_container_timeout_ms: 120000","4-copy-templates#4. Copy Templates":"Copy shared workflow files from the templates:\ncp agents/_templates/AGENTS.md agents/nova/AGENTS.md cp agents/_templates/DECISION.md agents/nova/DECISION.md cp agents/_templates/PULSE.md agents/nova/PULSE.md Then customize AGENTS.md with role-specific procedures. Replace the template placeholders with Novaâ€™s workflow.","5-restart#5. Restart":"docker compose restart engine The new agent will appear in the dashboard and can be used in pipelines and chat sessions.","agent-specific-skills#Agent-Specific Skills":"Create skills scoped to your agent:\nmkdir agents/nova/skills Add skill files like agents/nova/skills/dbt-models.md:\n--- name: dbt-models description: Building and testing dbt models tags: [dbt, sql, data-modeling, transform] enabled: true --- # dbt Models Skill ## When to Use When building or modifying dbt models for data transformation... These skills are only available to Nova, not other agents.","be-specific#Be Specific":"Bad: â€œI value quality code.â€\nGood: â€œIâ€™ve rewritten codebases twice because of sloppy type handling in data transforms. Now I use strict TypeScript everywhere and validate every input shape with Zod before it touches a pipeline.â€","define-collaboration-rules#Define Collaboration Rules":"Tell agents when to involve other team members:\n## Collaboration Triggers **Loop in Finn (SA) when:** - Data model changes affect the API - Performance implications are unclear **Loop in Chieko (QA) when:** - Data validation edge cases are complex - Need regression test strategy","include-productive-flaws#Include Productive Flaws":"Perfect agents are boring and unrealistic. Give them a characteristic weakness that creates interesting dynamics:\nEric cuts scope aggressively (sometimes too aggressively) Finn over-engineers (but catches problems others miss) Yukihiro refuses to ship without tests (even when deadlines are tight)","quick-start#Quick Start":"Create a new directory under agents/:\nmkdir agents/nova","tips-for-good-personas#Tips for Good Personas":"","using-custom-agents-in-pipelines#Using Custom Agents in Pipelines":"Reference your agent by ID in pipeline YAML:\nagents: - id: nova name: Nova (Data Engineer) tools: [read, write, bash] steps: - id: BUILD_PIPELINE agent: nova input: | Design and implement a data pipeline for: {{task_description}} outputs: [pipeline_design, implementation_notes]","write-anti-patterns#Write Anti-Patterns":"Things the agent refuses to do are as important as what they do:\n### I Will Not Ship Without Schema Validation I've had production data corrupted because a third-party API changed its response format silently. Now every external data source gets schema validation at the boundary."},"title":"Custom Agents"},"/docs/guides/custom-pipelines/":{"data":{"":"Create your own multi-agent workflows for any process â€” not just software development.","advanced-features#Advanced Features":"","loops#Loops":"Process a list of items with a single step:\n- id: IMPLEMENT agent: yukihiro input: | Current Task: {{current_item}} Completed: {{completed_items}} loop: over: task_breakdown_json # JSON array from previous step onEachComplete: REVIEW # Run after each item onAllComplete: FINALIZE # Run when all items done","minimal-pipeline#Minimal Pipeline":"Create a file in pipelines/:\n# pipelines/research.yml id: research name: Research Pipeline version: 1.0.0 description: Research a topic and produce a report defaults: model: anthropic/claude-sonnet-4 tools: [read, write, bash] timeout: 600 agents: - id: eric name: Eric (PO) steps: - id: RESEARCH agent: eric input: | Research this topic thoroughly: {{task_description}} Write a comprehensive report to REPORT.md. outputs: [report] Drop this file in pipelines/ and itâ€™s immediately available â€” no restart needed.","multi-step-with-handoffs#Multi-Step with Handoffs":"id: content name: Content Pipeline version: 1.0.0 description: Research, write, and review content defaults: model: openrouter/moonshotai/kimi-k2.5 tools: [read, write, bash] timeout: 900 agents: - id: luke name: Luke (SEO) - id: holt name: Holt (Marketing) - id: finn name: Finn (Reviewer) steps: - id: RESEARCH agent: luke input: | Research keywords and outline for: {{task_description}} Write outputs: - KEYWORD_RESEARCH.md - CONTENT_OUTLINE.md outputs: [keyword_research, content_outline] onComplete: WRITE - id: WRITE agent: holt input: | Write the content based on this research: {{keyword_research}} {{content_outline}} Write the full article to ARTICLE.md. outputs: [article] onComplete: REVIEW - id: REVIEW agent: finn input: | Review this content: {{article}} Output REVIEW_RESULT: APPROVED or CHANGES_REQUESTED outputs: [review_result, review_feedback] onResult: APPROVED: goto: DONE CHANGES_REQUESTED: goto: WRITE - id: DONE agent: luke input: | Finalize SEO metadata for: {{article}} outputs: [seo_metadata]","notifications#Notifications":"Notify agents when something important happens:\n- id: DEPLOY agent: stas onResult: SUCCESS: notify: agent: eric message: \"Deployment successful!\" FAIL: notify: agent: yukihiro message: \"Deployment failed, needs investigation\" goto: FIX","per-step-model-override#Per-Step Model Override":"Use different models for different steps:\n- id: SPEC agent: eric model: anthropic/claude-opus-4 # Expensive but thorough input: ... - id: IMPLEMENT agent: yukihiro model: openrouter/moonshotai/kimi-k2.5 # Fast and capable input: ...","pipeline-design-tips#Pipeline Design Tips":"Start simple â€” 2-3 steps is fine. Add complexity only when you need it. Use branching â€” let agents route work based on quality checks (APPROVED/CHANGES_REQUESTED). Set timeouts â€” prevent runaway steps from consuming resources. Enable retries â€” transient failures (API errors, etc.) are common. maxRetries: 3 handles them. Name outputs clearly â€” architecture_doc is better than output_1.","structured-output#Structured Output":"Force agents to produce valid JSON matching a schema:\n- id: PLAN agent: eric outputSchema: name: task_list strict: true schema: type: object properties: tasks: type: array items: type: object properties: title: { type: string } priority: { type: string, enum: [P0, P1, P2, P3] } required: [title, priority] required: [tasks] input: | Break down this project into tasks: {{task_description}} outputs: [task_list_json]"},"title":"Custom Pipelines"},"/docs/guides/github-integration/":{"data":{"":"DjinnBot integrates with GitHub for repository management, pull requests, and webhook-triggered automation.","agent-git-workflow#Agent Git Workflow":"","github-app-optional#GitHub App (Optional)":"For organization-level access and webhook support, configure a GitHub App:\nCreate a GitHub App at github.com/organizations/{org}/settings/apps/new Configure permissions: Repository: Contents (Read \u0026 Write), Issues (Read \u0026 Write), Pull Requests (Read \u0026 Write) Organization: Members (Read) Generate a private key and save it as secrets/github-app.pem Set environment variables: GITHUB_APP_ID=123456 GITHUB_APP_CLIENT_ID=Iv1.abc123 GITHUB_APP_WEBHOOK_SECRET=your-webhook-secret GITHUB_APP_NAME=djinnbot","mcp-github-tools#MCP GitHub Tools":"The default MCP configuration includes a GitHub tool server that gives agents read access to repositories, issues, and context:\n{ \"github\": { \"command\": \"/usr/local/bin/github-mcp-server\", \"args\": [\"stdio\", \"--read-only\", \"--toolsets\", \"context,repos,issues\"], \"env\": { \"GITHUB_PERSONAL_ACCESS_TOKEN\": \"${GITHUB_TOKEN}\" } } } Set GITHUB_TOKEN in your .env to enable this.","pipeline-sessions#Pipeline Sessions":"During pipeline runs, agents work in git worktrees:\nThe engine creates a worktree from the projectâ€™s repository The agent receives the workspace path via $WORKSPACE_PATH The agent reads, modifies, and creates files The engine commits and pushes after each step completes","pulse-sessions#Pulse Sessions":"During autonomous pulse work:\nAgent calls claim_task(projectId, taskId) â€” this provisions an authenticated git workspace on branch feat/{taskId} Agent works in /home/agent/task-workspaces/{taskId}/ Agent commits and pushes directly (credentials are pre-configured) Agent calls open_pull_request(projectId, taskId, title, body) when ready","webhooks#Webhooks":"The API server can receive GitHub webhooks at /v1/github/webhooks. Configure your GitHub App or repository webhook to point to:\nhttps://your-djinnbot-host:8000/v1/github/webhooks This enables triggering pipelines from GitHub events (issues, PRs, etc.)."},"title":"GitHub Integration"},"/docs/guides/projects/":{"data":{"":"DjinnBot includes built-in project management with kanban boards, task decomposition, and GitHub integration. Agents can autonomously pick up and work on tasks during pulse cycles.","autonomous-workflow-pulse-mode#Autonomous Workflow (Pulse Mode)":"When pulse mode is enabled, agents automatically:\nCheck their assigned columns for ready tasks Claim the highest-priority task Create a feature branch Implement the task Open a pull request Transition the task to Review This happens on a configurable schedule (default: every 30 minutes). See Pulse Mode for details.","creating-a-project#Creating a Project":"Go to Projects in the dashboard Click New Project Provide a name, description, and optionally a GitHub repository URL The project board is created with default columns: Backlog, Ready, In Progress, Review, Done","github-integration#GitHub Integration":"When a project is linked to a GitHub repository:\nTasks can be linked to GitHub issues Pull requests are automatically created when agents complete work Branch names follow the convention feat/task_{id}-{slug} Push access is configured via GitHub App or personal access token","manual-workflow#Manual Workflow":"Drag tasks between columns to manage work manually. Assign agents or yourself.","planning-pipeline#Planning Pipeline":"Use the planning pipeline to automatically decompose a project into tasks:\nOpen your project Click Plan Project (or start a planning pipeline run) Describe the project scope Eric (Product Owner) breaks it down into 5-20 tasks with priorities and dependencies Finn (Architect) validates the breakdown, fixing dependencies and estimates Eric decomposes into bite-sized subtasks (1-4 hours each) Finn validates the subtasks Tasks are automatically imported into the project board with:\nPriority labels (P0-P3) Dependency chains Hour estimates Tags (backend, frontend, devops, etc.)","setting-up-github-access#Setting Up GitHub Access":"Add a GitHub token to your .env:\nGITHUB_TOKEN=ghp_your_personal_access_token Or configure a GitHub App (for organization access):\nGITHUB_APP_ID=123456 GITHUB_APP_CLIENT_ID=Iv1.abc123 GITHUB_APP_WEBHOOK_SECRET=your-webhook-secret GITHUB_APP_PRIVATE_KEY_PATH=/data/secrets/github-app.pem Restart the services:\ndocker compose restart","task-dependencies#Task Dependencies":"Tasks can depend on other tasks. The dependency resolver ensures:\nTasks with unmet dependencies stay in Backlog When a dependency is completed, dependent tasks automatically move to Ready Circular dependencies are detected and flagged Dependencies are set during project planning (via the planning pipeline) or manually through the dashboard.","task-workflow#Task Workflow":"Tasks flow through the kanban board:\nBacklog â†’ Ready â†’ In Progress â†’ Review â†’ Done"},"title":"Project Management"},"/docs/guides/slack-setup/":{"data":{"":"Each DjinnBot agent can have its own Slack bot, appearing as a distinct team member in your workspace. This guide walks you through creating Slack apps for your agents.\nSlack integration is optional. The built-in dashboard chat works without any Slack configuration. Set this up only if you want agents visible in your Slack workspace.","bot-doesnt-respond-to-mentions#Bot doesn\u0026rsquo;t respond to mentions":"Verify the bot is invited to the channel Check that app_mention event subscription is enabled Confirm Socket Mode is enabled and tokens are correct Check engine logs: docker compose logs engine | grep -i slack","bot-posts-but-doesnt-show-agent-name#Bot posts but doesn\u0026rsquo;t show agent name":"Each agent needs its own Slack app â€” you canâ€™t use one bot for all agents Verify slack.yml exists for the agent with correct token references","customizing-bot-appearance#Customizing Bot Appearance":"In each Slack appâ€™s settings:\nGo to Basic Information â†’ Display Information Set the bot name (e.g., â€œEric - Product Ownerâ€) Upload a profile photo (agent avatar) Set a description and background color","dms#DMs":"Send a DM to any agent bot for a private conversation. Useful for quick questions or ad-hoc tasks.","how-slack-integration-works#How Slack Integration Works":"","mentions#Mentions":"Mention an agent in any channel itâ€™s in (@Eric what do you think about this feature?) and it will respond in character, using its full persona and memory.","overview#Overview":"Each agent needs its own Slack app because:\nAgents post messages under their own name and avatar Each agent can be mentioned independently (@Eric, @Finn, etc.) Socket Mode allows real-time bidirectional communication Each bot has its own OAuth scopes and permissions Youâ€™ll create one Slack app per agent, then add the tokens to your .env file.","pipeline-threads#Pipeline Threads":"When a pipeline run starts, the engine creates a thread in the configured channel. Each stepâ€™s output is posted as replies in the thread, attributed to the agent handling that step. You can watch a full engineering pipeline unfold as a Slack conversation.","slack-resources#Slack Resources":"Slack API Documentation Socket Mode Guide Bot Token Scopes Reference Agents \u0026 Assistants â€” Slackâ€™s AI bot framework Slack App Manifest â€” automate app creation","socket-mode-disconnects#Socket Mode disconnects":"Socket Mode tokens expire â€” regenerate if needed Check network connectivity between the engine container and wss://wss-primary.slack.com","step-1-create-a-slack-app#Step 1: Create a Slack App":"For each agent you want in Slack:\nGo to api.slack.com/apps Click Create New App â†’ From scratch Name it after the agent (e.g., â€œEric - Product Ownerâ€) Select your workspace Click Create App Slackâ€™s AI features: Slack has recently released AI-powered features for bots including Agents \u0026 Assistants. DjinnBot agents use Socket Mode which is the recommended approach for internal tools and works within Slackâ€™s AI framework. See Slackâ€™s Agents \u0026 Assistants documentation for the latest on building AI-powered Slack experiences.","step-2-configure-oauth-permissions#Step 2: Configure OAuth Permissions":"Navigate to OAuth \u0026 Permissions and add these Bot Token Scopes:\nScope Purpose app_mentions:read Detect when the agent is @mentioned channels:history Read messages in public channels channels:read List channels chat:write Send messages files:read Access shared files files:write Upload files groups:history Read private channel messages groups:read List private channels im:history Read DMs im:read List DMs im:write Send DMs reactions:write Add emoji reactions users:read Look up user info","step-3-enable-socket-mode#Step 3: Enable Socket Mode":"Navigate to Socket Mode in the sidebar:\nToggle Enable Socket Mode to ON Give the app-level token a name (e.g., â€œeric-socketâ€) Add the connections:write scope Click Generate Save the App-Level Token (xapp-...) â€” this is the APP_TOKEN Socket Mode lets the bot connect without exposing a public URL, which is ideal for self-hosted deployments behind firewalls.","step-4-enable-events#Step 4: Enable Events":"Navigate to Event Subscriptions:\nToggle Enable Events to ON\nUnder Subscribe to bot events, add:\napp_mention â€” responds when someone @mentions the agent message.channels â€” sees messages in channels itâ€™s in message.groups â€” sees messages in private channels message.im â€” receives DMs Click Save Changes","step-5-install-to-workspace#Step 5: Install to Workspace":"Navigate to Install App:\nClick Install to Workspace Review the permissions and approve Copy the Bot User OAuth Token (xoxb-...) â€” this is the BOT_TOKEN","step-6-add-tokens-to-djinnbot#Step 6: Add Tokens to DjinnBot":"Edit your .env file and add the tokens for each agent:\n# Slack channel where agents post pipeline updates SLACK_CHANNEL_ID=C0123456789 # Eric (Product Owner) SLACK_ERIC_BOT_TOKEN=xoxb-your-eric-bot-token SLACK_ERIC_APP_TOKEN=xapp-your-eric-app-token # Finn (Solutions Architect) SLACK_FINN_BOT_TOKEN=xoxb-your-finn-bot-token SLACK_FINN_APP_TOKEN=xapp-your-finn-app-token # Add more agents as needed... The naming convention is SLACK_{AGENT_ID_UPPERCASE}_BOT_TOKEN and SLACK_{AGENT_ID_UPPERCASE}_APP_TOKEN.\nAlso create agents//slack.yml for each agent:\nbot_token: ${SLACK_ERIC_BOT_TOKEN} app_token: ${SLACK_ERIC_APP_TOKEN}","step-7-set-up-the-channel#Step 7: Set Up the Channel":"Create a channel for your DjinnBot team (e.g., #djinnbot-dev) Invite each agent bot to the channel Copy the channel ID (right-click channel name â†’ Copy Link, the ID is the C... part) Set SLACK_CHANNEL_ID in .env","step-8-restart#Step 8: Restart":"docker compose down \u0026\u0026 docker compose up -d The engine will connect each agent to Slack via Socket Mode on startup.","thread-modes#Thread Modes":"Agents can operate in different thread modes:\npassive â€” only responds when directly mentioned active â€” proactively participates in conversations (watches for relevant topics) Configure via thread_mode in the agentâ€™s config.yml.","troubleshooting#Troubleshooting":""},"title":"Slack Bot Setup"},"/docs/reference/":{"data":{"":"Complete reference documentation for DjinnBotâ€™s API, CLI, configuration, and environment variables.\nAPI ReferenceREST endpoints for runs, agents, projects, memory, and more. CLI ReferenceCommand-line interface for pipeline and agent management. ConfigurationEnvironment variables and settings. Pipeline YAMLComplete reference for pipeline definition syntax."},"title":"Reference"},"/docs/reference/api/":{"data":{"":"The DjinnBot API server runs at http://localhost:8000 and provides a REST API for all operations. All endpoints are prefixed with /v1/.","agents#Agents":"GET /v1/agents # List all agents GET /v1/agents/{id} # Get agent details GET /v1/agents/{id}/runs # Get agent run history PUT /v1/agents/{id}/config # Update agent configuration","chat#Chat":"POST /v1/chat/sessions # Create a chat session GET /v1/chat/sessions # List active sessions GET /v1/chat/sessions/{id} # Get session details POST /v1/chat/sessions/{id}/message # Send a message DELETE /v1/chat/sessions/{id} # End a session","create-run-request#Create Run Request":"{ \"pipeline_id\": \"engineering\", \"task_description\": \"Build a REST API for a todo app\", \"human_context\": \"Optional additional guidance\", \"project_name\": \"my-project\" }","events-sse#Events (SSE)":"GET /v1/events/stream # Server-Sent Events stream ?run_id=run_123 # Filter to a specific run Returns real-time events including:\nrun_created, run_complete, run_failed step_queued, step_started, step_complete, step_failed agent_output (streaming text chunks) agent_thinking (reasoning blocks) tool_call_start, tool_call_end","github#GitHub":"POST /v1/github/webhooks # Receive GitHub webhooks GET /v1/github/repos # List accessible repos GET /v1/github/app/status # GitHub App connection status","lifecycle#Lifecycle":"GET /v1/lifecycle/timeline # Agent activity timeline GET /v1/lifecycle/sessions # Active sessions","mcp#MCP":"GET /v1/mcp # List MCP servers GET /v1/mcp/{id} # Get server details POST /v1/mcp # Register a new server PUT /v1/mcp/{id} # Update server config DELETE /v1/mcp/{id} # Remove a server GET /v1/mcp/config.json # Get merged config PATCH /v1/mcp/{id}/status # Update server status PATCH /v1/mcp/{id}/tools # Update discovered tools GET /v1/mcp/logs # Stream MCP proxy logs","memory#Memory":"GET /v1/memory/vaults # List all vaults GET /v1/memory/vaults/{agent_id} # Get vault contents GET /v1/memory/search # Search memories ?agent_id=eric\u0026query=architecture\u0026limit=5 GET /v1/memory/shared # Search shared knowledge","pipelines#Pipelines":"GET /v1/pipelines # List all pipeline definitions GET /v1/pipelines/{id} # Get a specific pipeline","projects#Projects":"GET /v1/projects # List projects POST /v1/projects # Create a project GET /v1/projects/{id} # Get project details GET /v1/projects/{id}/tasks # List tasks POST /v1/projects/{id}/tasks # Create a task PUT /v1/projects/{id}/tasks/{tid} # Update a task POST /v1/projects/{id}/tasks/{tid}/claim # Claim a task POST /v1/projects/{id}/tasks/{tid}/transition # Move task","runs#Runs":"GET /v1/runs # List runs (?pipeline_id= filter) GET /v1/runs/{id} # Get run details POST /v1/runs # Create a new run POST /v1/runs/{id}/cancel # Cancel a running pipeline POST /v1/runs/{id}/restart # Restart a failed run","secrets#Secrets":"GET /v1/secrets # List secrets (names only) POST /v1/secrets # Store a secret DELETE /v1/secrets/{name} # Delete a secret","settings#Settings":"GET /v1/settings # Get all settings PUT /v1/settings # Update settings GET /v1/settings/providers # List LLM providers PUT /v1/settings/providers/{id} # Update provider config GET /v1/settings/providers/keys/all # Get all provider API keys","skills#Skills":"GET /v1/skills # List all skills GET /v1/skills/{name} # Get skill content POST /v1/skills # Create a skill PUT /v1/skills/{name} # Update a skill DELETE /v1/skills/{name} # Delete a skill PATCH /v1/skills/{name}/enabled # Toggle enabled state","status#Status":"GET /v1/status Returns server health, Redis connection status, and summary statistics.","steps#Steps":"GET /v1/steps/{run_id} # List steps for a run GET /v1/steps/{run_id}/{step_id} # Get step details GET /v1/steps/{run_id}/{step_id}/output # Get step output"},"title":"API Reference"},"/docs/reference/cli/":{"data":{"":"The DjinnBot CLI provides command-line access to all platform features.","agents#Agents":"# List all agents djinnbot agent list # Show agent details djinnbot agent show eric # View agent run history djinnbot agent runs eric","commands#Commands":"","installation#Installation":"cd cli pip install -e . djinnbot --help The CLI connects to the API server at http://localhost:8000 by default. Override with --api-url or the DJINNBOT_API_URL environment variable.","memory#Memory":"# List vaults djinnbot memory list-vaults # Search agent memory djinnbot memory search eric \"architecture decisions\" # View vault contents djinnbot memory vault eric # Search shared knowledge djinnbot memory shared \"deployment patterns\"","output-format#Output Format":"The CLI uses Rich for terminal formatting â€” tables, syntax highlighting, progress bars, and colored output. Pipe to a file or use --json for machine-readable output.","pipelines#Pipelines":"# List all pipelines djinnbot pipeline list # Show pipeline details djinnbot pipeline show engineering # Start a new run djinnbot pipeline start engineering \\ --task \"Build a task management CLI tool in Python\"","runs#Runs":"# List recent runs djinnbot run list # Show run details djinnbot run show # Stream run output in real-time djinnbot run stream # Cancel a running pipeline djinnbot run cancel # Restart a failed run djinnbot run restart","status#Status":"djinnbot status Show server health, Redis connection, and summary statistics.","steps#Steps":"# List steps for a run djinnbot step list # Show step details djinnbot step show # View step output djinnbot step output"},"title":"CLI Reference"},"/docs/reference/configuration/":{"data":{"":"All configuration is done through environment variables in .env and per-agent config.yml files.","agent-configuration-configyml#Agent Configuration (config.yml)":"Per-agent settings in agents//config.yml:\n# LLM Model model: anthropic/claude-sonnet-4 thinking_model: anthropic/claude-sonnet-4 thinking_level: 'off' # off, low, medium, high # Slack thread_mode: passive # passive or active # Pulse (autonomous mode) pulse_enabled: false pulse_interval_minutes: 30 pulse_offset_minutes: 3 pulse_max_consecutive_skips: 5 pulse_container_timeout_ms: 120000 pulse_columns: - Backlog - Ready pulse_transitions_to: - planning - ready - in_progress pulse_blackouts: - label: Nighttime start_time: '23:00' end_time: '07:00' type: recurring pulse_one_offs: [] All agent config can also be edited through the dashboard Settings and Agent pages.","engine#Engine":"Variable Default Description MOCK_RUNNER false Use mock agent runner for testing USE_CONTAINER_RUNNER true Use Docker containers for agents LOG_LEVEL INFO Logging level VITE_API_URL http://192.168.8.234:8000 API URL baked into dashboard build","environment-variables#Environment Variables":"","github#GitHub":"Variable Description GITHUB_TOKEN Personal access token for git operations GITHUB_APP_ID GitHub App ID GITHUB_APP_CLIENT_ID GitHub App client ID GITHUB_APP_WEBHOOK_SECRET Webhook signature verification GITHUB_APP_PRIVATE_KEY_PATH Path to App private key PEM GITHUB_APP_NAME GitHub App name","global-settings-dashboard#Global Settings (Dashboard)":"The Settings page in the dashboard provides UI access to:\nDefault working model â€” model for pipeline steps and chat Default thinking model â€” model for extended reasoning Pulse interval â€” global pulse frequency Provider API keys â€” add/update provider credentials Custom providers â€” configure OpenAI-compatible endpoints","llm-providers#LLM Providers":"Variable Provider OPENROUTER_API_KEY OpenRouter (access to all models) ANTHROPIC_API_KEY Anthropic (Claude) OPENAI_API_KEY OpenAI (GPT) GEMINI_API_KEY Google (Gemini) XAI_API_KEY xAI (Grok) GROQ_API_KEY Groq MISTRAL_API_KEY Mistral CEREBRAS_API_KEY Cerebras AZURE_OPENAI_API_KEY Azure OpenAI HF_TOKEN Hugging Face","paths#Paths":"Variable Default Description PIPELINES_DIR ./pipelines Pipeline YAML directory AGENTS_DIR ./agents Agent persona directory DATA_DIR ./data General data directory","required#Required":"Variable Description Example OPENROUTER_API_KEY OpenRouter API key (also used for memory embeddings) sk-or-v1-... Thatâ€™s the only required variable when using OpenRouter. Everything else has defaults.","security#Security":"Variable Description SECRET_ENCRYPTION_KEY AES-256-GCM key for encrypting secrets at rest MCPO_API_KEY API key protecting the mcpo proxy","services#Services":"Variable Default Description API_PORT 8000 API server port DASHBOARD_PORT 3000 Dashboard port REDIS_PORT 6379 Redis port POSTGRES_PORT 5432 PostgreSQL port MCPO_PORT 8001 MCP proxy port","slack#Slack":"Variable Description SLACK_CHANNEL_ID Default channel for pipeline threads SLACK_{AGENT}_BOT_TOKEN Per-agent Slack bot token SLACK_{AGENT}_APP_TOKEN Per-agent Slack app token SKY_SLACK_USER_ID Human user ID for DM notifications"},"title":"Configuration"},"/docs/reference/pipeline-yaml/":{"data":{"":"Complete reference for the pipeline definition format.","available-tools#Available Tools":"Tools that can be listed in the tools array:\nTool Description read Read files from the workspace write Write files to the workspace edit Edit files with search/replace bash Execute shell commands grep Search file contents find Find files by pattern ls List directory contents web_search Search the web (via research tool) Agents also have access to built-in tools (memory, messaging, project management) that are always available regardless of the tools list.","loop-result-routing#Loop Result Routing":"Within a loop, use continueLoop: true to advance to the next item:\nonResult: PASS: continueLoop: true # Move to next loop item FAIL: goto: IMPLEMENT # Go back to fix","result-routing#Result Routing":"The onResult field maps agent output values to step transitions:\nonResult: APPROVED: goto: TEST CHANGES_REQUESTED: goto: IMPLEMENT FAIL: goto: FIX notify: agent: eric message: \"Test failed, needs investigation\" The agent writes the result value to a file named {OUTPUT}_RESULT.txt (e.g., REVIEW_RESULT.txt containing APPROVED).","step-fields#Step Fields":"- id: string # Unique step identifier (e.g., SPEC, DESIGN) agent: string # Agent ID that handles this step input: | # Prompt template (supports {{variables}} and {% if %}) You are the Product Owner. Task: {{task_description}} outputs: # Named outputs the agent produces - product_brief - requirements_doc # Flow control (use exactly one) onComplete: string # Next step ID when this step succeeds onResult: # Conditional routing based on agent output APPROVED: goto: string # Go to step CHANGES_REQUESTED: goto: string notify: # Optional notification agent: string message: string # Loop execution loop: over: string # Output name containing JSON array to iterate onEachComplete: string # Step to run after each item onAllComplete: string # Step to run after all items # Overrides model: string # Override default model for this step timeout: number # Override default timeout # Structured output (mutually exclusive with tools) outputSchema: name: string strict: boolean schema: # JSON Schema object type: object properties: ... required: [...]","template-variables#Template Variables":"Available in input fields:\nVariable Source {{task_description}} From the run creation request {{human_context}} Optional human guidance from run request {{project_name}} Project name from run request {{output_name}} Any named output from a previous step {{current_item}} Current item in a loop iteration {{completed_items}} JSON array of completed loop items {{progress_file}} Path to loop progress tracking file Jinja2 conditionals:\ninput: | {% if review_feedback %} Address this feedback: {{review_feedback}} {% endif %}","top-level-fields#Top-Level Fields":"id: string # Unique pipeline identifier name: string # Display name version: string # Semantic version description: string # Pipeline description defaults: # Default values applied to all steps model: string # Default LLM model tools: [string] # Default tool list maxRetries: number # Default retry count (default: 0) timeout: number # Default timeout in seconds agents: # Agent declarations - id: string name: string persona: string # Path to persona files model: string # Override default model tools: [string] # Override default tools steps: # Pipeline step definitions - ..."},"title":"Pipeline YAML Reference"},"/glossary":{"data":{},"title":"Glossary"}}